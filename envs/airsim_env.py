import airsim
import numpy as np
import time
import random


class AirSimUAVEnv:
    def __init__(self):
        self.client = airsim.MultirotorClient()
        self.client.confirmConnection()
        self.client.enableApiControl(True)
        self.client.armDisarm(True)

        self.action_duration = 0.4
        self.max_episode_steps = 1000
        self.goal_radius = 2.0
        self.step_count = 0
        self.trajectory = []

        # ğŸ”§ æ–°å¢ï¼šå¡ä½æ£€æµ‹æœºåˆ¶
        self.stuck_history = []  # è®°å½•æœ€è¿‘å‡ æ­¥çš„ä½ç½®å’Œé€Ÿåº¦
        self.stuck_threshold = 10  # è¿ç»­5æ­¥æ£€æµ‹å¡ä½
        self.min_altitude = -1.0  # æœ€å°é£è¡Œé«˜åº¦
        self.max_stuck_attempts = 3  # æœ€å¤§é‡æ–°èµ·é£å°è¯•æ¬¡æ•°

    def reset(self):
        self.client.reset()
        self.client.enableApiControl(True)
        self.client.armDisarm(True)
        self.client.takeoffAsync().join()
        time.sleep(0.5)
        self.client.moveToZAsync(-10, velocity=2).join()

        # ğŸ¯ è®¾ç½®ç›®æ ‡ç‚¹ï¼š65 ç±³åŠå¾„åœ†å‘¨éšæœºä½ç½®
        theta = random.uniform(0, 2 * np.pi)
        r = 35
        self.goal_pos = np.array([r * np.cos(theta), r * np.sin(theta), -10.0])
        self.prev_distance = self._compute_distance_to_goal()

        # âœ… æ¸…ç©ºè½¨è¿¹å’Œå¡ä½å†å²
        self.step_count = 0
        self.trajectory = []
        self.stuck_history = []
        self.client.simFlushPersistentMarkers()

        return self._get_obs()

    def step(self, action):
        # 1. æ‹¿å›æ§åˆ¶ & è§£é”
        self.client.enableApiControl(True)
        self.client.armDisarm(True)

        # 2. æ‰§è¡ŒåŠ¨ä½œ
        vx, vy, vz, yaw_rate = [float(a) for a in action]
        self.client.moveByVelocityAsync(
            vx, vy, vz,
            duration=self.action_duration,
            drivetrain=airsim.DrivetrainType.MaxDegreeOfFreedom,
            yaw_mode=airsim.YawMode(is_rate=True, yaw_or_rate=yaw_rate)
        ).join()
        self.client.moveToZAsync(-10, velocity=5).join()

        # 3. å–æ–°è§‚æµ‹
        next_obs = self._get_obs()

        # 4. æ£€æµ‹æ˜¯å¦â€œå®‰å…¨æ‚¬åœâ€å¯¼è‡´è½åœ°ä¸åŠ¨
        state = self.client.getMultirotorState()
        pos = state.kinematics_estimated.position
        v = state.kinematics_estimated.linear_velocity
        speed = np.linalg.norm([v.x_val, v.y_val, v.z_val])
        dist_to_goal = np.linalg.norm(
            np.array([pos.x_val, pos.y_val, pos.z_val]) - self.goal_pos
        )

        # å¦‚æœé€Ÿåº¦æ¥è¿‘ 0ã€ä¸å¹³å°é«˜åº¦ç›¸å½“ï¼Œä¸”è¿˜æ²¡åˆ°ç›®æ ‡ â†’ ç›´æ¥å½“ä½œ done
        if speed < 0.1 and abs(pos.z_val + 10) < 1.0 and dist_to_goal > self.goal_radius:
            # è¿™é‡Œç»™ä¸€ä¸ªè¾ƒå¤§çš„è´Ÿå¥–åŠ±ï¼Œè¿«ä½¿ agent ä¸è¦æŠŠè‡ªå·±å¡ä½
            stuck_reward = -50
            print(f"ğŸ›‘ Detected hover/stuck (speed={speed:.2f}, z={pos.z_val:.2f}), ending episode.")
            return next_obs, stuck_reward, True

        # 5. æ­£å¸¸è®¡ç®— reward å’Œ done
        reward, done = self._compute_reward()
        self.step_count += 1

        # 6. Episode è¶…æ—¶æˆ–åˆ°è¾¾ç›®æ ‡æ—¶ç»˜å›¾å¹¶ç»“æŸ
        if self.step_count >= self.max_episode_steps or done:
            self.client.simPlotPoints(
                self.trajectory, color_rgba=[1,0,0,1], size=10.0, is_persistent=True
            )
            self.client.simPlotPoints(
                [airsim.Vector3r(*self.goal_pos)], color_rgba=[0,1,0,1], size=30.0, is_persistent=True
            )
            done = True

        return next_obs, reward, done

    def _is_stuck(self):
        """ğŸ”§ æ”¹è¿›çš„å¡ä½æ£€æµ‹æœºåˆ¶"""
        state = self.client.getMultirotorState()
        pos = state.kinematics_estimated.position
        vel = state.kinematics_estimated.linear_velocity

        current_state = {
            'position': np.array([pos.x_val, pos.y_val, pos.z_val]),
            'velocity': np.array([vel.x_val, vel.y_val, vel.z_val]),
            'altitude': pos.z_val,
            'speed': np.linalg.norm([vel.x_val, vel.y_val, vel.z_val])
        }

        # æ·»åŠ åˆ°å†å²è®°å½•
        self.stuck_history.append(current_state)
        if len(self.stuck_history) > self.stuck_threshold:
            self.stuck_history.pop(0)

        # éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
        if len(self.stuck_history) < self.stuck_threshold:
            return False

        # æ£€æµ‹æ¡ä»¶1ï¼šé«˜åº¦è¿‡ä½
        if current_state['altitude'] > self.min_altitude:
            return True

        # æ£€æµ‹æ¡ä»¶2ï¼šé€Ÿåº¦æŒç»­è¿‡ä½
        recent_speeds = [s['speed'] for s in self.stuck_history]
        if all(speed < 0.3 for speed in recent_speeds):
            return True

        # æ£€æµ‹æ¡ä»¶3ï¼šä½ç½®å‡ ä¹æ²¡æœ‰å˜åŒ–
        recent_positions = [s['position'] for s in self.stuck_history]
        position_changes = []
        for i in range(1, len(recent_positions)):
            change = np.linalg.norm(recent_positions[i] - recent_positions[i - 1])
            position_changes.append(change)

        if all(change < 0.5 for change in position_changes):
            return True

        return False

    def _attempt_recovery(self):
        """ğŸ”§ é£æœºå¡ä½åçš„æ¢å¤æœºåˆ¶"""
        for attempt in range(self.max_stuck_attempts):
            print(f"ğŸ”„ Recovery attempt {attempt + 1}/{self.max_stuck_attempts}")

            try:
                # Step 1: é‡æ–°å¯ç”¨æ§åˆ¶
                self.client.enableApiControl(True)
                self.client.armDisarm(True)
                time.sleep(0.2)

                # Step 2: å¼ºåˆ¶è®¾ç½®åˆ°å®‰å…¨ä½ç½®
                current_pos = self.client.getMultirotorState().kinematics_estimated.position
                safe_pose = airsim.Pose(
                    airsim.Vector3r(current_pos.x_val, current_pos.y_val, -10),
                    airsim.to_quaternion(0, 0, 0)
                )
                self.client.simSetVehiclePose(safe_pose, ignore_collision=True)
                time.sleep(0.3)

                # Step 3: é‡æ–°èµ·é£
                self.client.takeoffAsync().join()
                time.sleep(0.5)

                # Step 4: ç¡®ä¿åˆ°è¾¾ç›®æ ‡é«˜åº¦
                self.client.moveToZAsync(-10, velocity=3).join()
                time.sleep(0.3)

                # Step 5: å”¤é†’ UAVï¼ˆç¡®ä¿èºæ—‹æ¡¨è½¬åŠ¨ï¼‰
                print("âœ¨ Sending small move command to exit hover...")
                self.client.moveByVelocityAsync(0.1, 0.0, 0.0, duration=0.5).join()

                # Step 6: æ§åˆ¶çŠ¶æ€ç¡®è®¤
                print(f"ğŸ§ª isApiControlEnabled = {self.client.isApiControlEnabled()}")

                # Step 5: éªŒè¯æ¢å¤æ˜¯å¦æˆåŠŸ
                new_state = self.client.getMultirotorState()
                new_pos = new_state.kinematics_estimated.position
                new_vel = new_state.kinematics_estimated.linear_velocity
                new_speed = np.linalg.norm([new_vel.x_val, new_vel.y_val, new_vel.z_val])

                print(f"ğŸ“ After recovery: altitude={new_pos.z_val:.2f}, speed={new_speed:.2f}")

                # éªŒè¯æ¢å¤æˆåŠŸçš„æ¡ä»¶
                if new_pos.z_val < -7 and abs(new_pos.z_val - (-10)) < 2:
                    print("âœ… Recovery successful!")
                    self.stuck_history = []  # æ¸…ç©ºå†å²è®°å½•
                    return True

            except Exception as e:
                print(f"âŒ Recovery attempt {attempt + 1} failed: {e}")
                continue

        print("âŒ All recovery attempts failed")
        return False

    def _get_obs(self):
        # ğŸ¥ è·å–æ·±åº¦å›¾
        responses = self.client.simGetImages([
            airsim.ImageRequest("0", airsim.ImageType.DepthPerspective, pixels_as_float=True)
        ])
        depth = np.array(responses[0].image_data_float, dtype=np.float32)

        # ğŸ§­ è·å–çŠ¶æ€
        state = self.client.getMultirotorState()
        pos = state.kinematics_estimated.position
        vel = state.kinematics_estimated.linear_velocity

        obs_vector = np.array([
            pos.x_val, pos.y_val, pos.z_val,
            vel.x_val, vel.y_val, vel.z_val,
            0.0,  # yaw_err
            0.0  # yaw_rate
        ])

        # âœ… æ¯æ­¥è®°å½• UAV ä½ç½®
        self.trajectory.append(airsim.Vector3r(pos.x_val, pos.y_val, pos.z_val))

        return depth, obs_vector

    def _compute_reward(self):
        current_pos = self._get_position()
        curr_dist = np.linalg.norm(current_pos - self.goal_pos)
        progress_reward = (self.prev_distance - curr_dist) / 35.0
        distance_penalty = -curr_dist / 35.0

        reward = 20.0 * progress_reward + 2.0 * distance_penalty

        # ğŸ”§ æ–°å¢ï¼šé«˜åº¦æƒ©ç½šï¼ˆé˜²æ­¢é£æœºé£å¾—å¤ªä½ï¼‰
        altitude_penalty = 0
        if current_pos[2] > self.min_altitude:
            altitude_penalty = -5.0 * (current_pos[2] - self.min_altitude)

        reward += altitude_penalty
        done = False

        # âŒ ç¢°æ’æƒ©ç½š
        collision_info = self.client.simGetCollisionInfo()
        if collision_info.has_collided:
            reward -= 10

        # âœ… æˆåŠŸåˆ°è¾¾ç›®æ ‡ç‚¹
        if curr_dist < self.goal_radius:
            reward += 10000
            done = True

        self.prev_distance = curr_dist
        return reward, done

    def _get_position(self):
        pos = self.client.getMultirotorState().kinematics_estimated.position
        return np.array([pos.x_val, pos.y_val, pos.z_val])

    def _compute_distance_to_goal(self):
        return np.linalg.norm(self._get_position() - self.goal_pos)

    def get_trajectory(self):
        # è¿”å›å½“å‰ episode çš„ UAV è·¯å¾„åæ ‡ [(x1,y1,z1), ...]
        return [(p.x_val, p.y_val, p.z_val) for p in self.trajectory]