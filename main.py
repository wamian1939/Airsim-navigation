import numpy as np
import torch
from envs.airsim_env import AirSimUAVEnv
from models.actor import Actor
from models.critic import Critic
from replay_buffer import ReplayBuffer
from td3_trainer import TD3Trainer
import matplotlib.pyplot as plt
from tqdm import tqdm
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# åˆå§‹åŒ–ç¯å¢ƒ
env = AirSimUAVEnv()
state_dim = 8
action_dim = 4
max_action = 15.0
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆå§‹åŒ–ç½‘ç»œ & Trainer
actor = Actor(state_dim, action_dim, max_action).to(device)
critic = Critic(state_dim, action_dim).to(device)
trainer = TD3Trainer(state_dim, action_dim, max_action, actor, critic)

# åˆå§‹åŒ–ç»éªŒæ± 
replay_buffer = ReplayBuffer(state_dim, action_dim, max_size=50000)

# è®­ç»ƒå‚æ•°
total_episodes = 50
max_steps = 1000
start_training_after = 500
train_every = 2

# ä¿å­˜æ¯ä¸ª episode çš„æ€»å¥–åŠ±
all_rewards = []

# è®­ç»ƒä¸»å¾ªç¯
for episode in tqdm(range(total_episodes), desc="Training Progress"):
    print(f"\nğŸš Episode {episode + 1}")
    depth, state = env.reset()
    episode_reward = 0
    progress_bar = tqdm(range(max_steps), desc=f"Episode {episode + 1}/{total_episodes}", leave=False)

    for step in progress_bar:
        # âœ… reshape depth â†’ [1, 1, 256, 144]
        depth = depth.reshape(256, 144)[np.newaxis, :, :]
        depth_tensor = torch.FloatTensor(depth).unsqueeze(0).to(device)
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

        # åŠ¨ä½œé€‰æ‹©
        if replay_buffer.size < start_training_after:
            action = np.random.uniform(-max_action, max_action, size=action_dim)
            action[2] = 0.0  # é”æ­» vz
        else:
            actor.eval()
            with torch.no_grad():
                action = actor(depth_tensor, state_tensor).cpu().numpy().flatten()
            action += np.random.normal(0, 0.5, size=action_dim)
            action = np.clip(action, -max_action, max_action)
            action[2] = 0.0  # åªåœ¨å¹³é¢ç§»åŠ¨

        # ä¸ç¯å¢ƒäº¤äº’
        (next_depth, next_state), reward, done = env.step(action)

        # âœ… reshape å½“å‰å’Œä¸‹ä¸€ä¸ª depth ä¸º [1, H, W]
        depth = depth.reshape(256, 144)[np.newaxis, :, :]
        next_depth = next_depth.reshape(256, 144)[np.newaxis, :, :]

        # å­˜å…¥ç»éªŒæ± 
        replay_buffer.add(state, action, reward, next_state, done, depth, next_depth)

        # æ›´æ–°çŠ¶æ€
        state = next_state
        depth = next_depth
        episode_reward += reward

        progress_bar.set_postfix(reward=f"{reward:.2f}", total=f"{episode_reward:.2f}")

        # å¼€å§‹è®­ç»ƒ
        if replay_buffer.size >= start_training_after:
            for _ in range(train_every):
                trainer.train(replay_buffer, batch_size=128)

        if done:
            break


    all_rewards.append(episode_reward)
    print(f"ğŸ¯ Reward: {episode_reward:.2f} | Steps: {step + 1} | Buffer: {replay_buffer.size}")
    print("depth shape:", depth.shape)
    trajectory = env.get_trajectory()
    xs, ys, zs = zip(*trajectory)
    plt.figure(figsize=(6, 6))
    plt.plot(xs, ys, marker='o', markersize=2, label="UAV path", color='blue')
    plt.scatter([env.goal_pos[0]], [env.goal_pos[1]], color='green', s=80, label="Goal")
    plt.scatter([xs[0]], [ys[0]], color='red', s=80, label="Start")
    plt.title(f"Trajectory - Episode {episode + 1}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.axis('equal')
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"trajectory_ep{episode + 1}.png")
    plt.close()
    print(f"ğŸ–¼ï¸ Trajectory plot saved to: trajectory_ep{episode + 1}.png")




# âœ… æ‰€æœ‰ episodes è®­ç»ƒå®Œä¹‹åå†ç”»ä¸€æ¬¡å›¾
plt.figure()
plt.plot(all_rewards, label="Episode Reward")
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("TD3 Training Reward Curve")
plt.legend()
plt.grid()
plt.savefig("reward_curve.png")
print("âœ… Reward curve saved to reward_curve.png")
