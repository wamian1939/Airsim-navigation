# main.py

import numpy as np
import torch
from envs.airsim_env import AirSimUAVEnv
from models.actor import Actor
from models.critic import Critic
from replay_buffer import ReplayBuffer
from td3_trainer import TD3Trainer
import matplotlib.pyplot as plt


# åˆå§‹åŒ–ç¯å¢ƒ
env = AirSimUAVEnv()
state_dim = 8
action_dim = 4
max_action = 10.0

all_rewards = []


# åˆå§‹åŒ–ç½‘ç»œ & Trainer
actor = Actor(state_dim, action_dim, max_action)
critic = Critic(state_dim, action_dim)
trainer = TD3Trainer(state_dim, action_dim, max_action, actor, critic)

# åˆå§‹åŒ–ç»éªŒæ± 
replay_buffer = ReplayBuffer(state_dim, action_dim, max_size=50000)

# è®­ç»ƒå‚æ•°
total_episodes = 1000
max_steps = 500
start_training_after = 10000  # æ”¶é›†å¤šå°‘ç»éªŒåå¼€å§‹è®­ç»ƒ
train_every = 5               # æ¯æ­¥è®­ç»ƒå‡ æ¬¡

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for episode in range(total_episodes):
    print(f"\nğŸš Episode {episode + 1}")
    depth, state = env.reset()
    episode_reward = 0

    for step in range(max_steps):
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

        # åŠ¨ä½œé€‰æ‹©ï¼ˆæœ‰æ¢ç´¢å™ªå£°ï¼‰
        if replay_buffer.size < start_training_after:
            action = np.random.uniform(-max_action, max_action, size=action_dim)
            action[2] = 0.0
        else:
            actor.eval()
            with torch.no_grad():
                action = actor(state_tensor).cpu().numpy().flatten()
            action += np.random.normal(0, 0.5, size=action_dim)  # æ¢ç´¢å™ªå£°
            action = np.clip(action, -max_action, max_action)
            action[2] = 0.0

        (next_depth, next_state), reward, done = env.step(action)
        replay_buffer.add(state, action, reward, next_state, done)

        state = next_state
        episode_reward += reward
        all_rewards.append(episode_reward)


        # å¼€å§‹è®­ç»ƒ
        if replay_buffer.size >= start_training_after:
            for _ in range(train_every):
                trainer.train(replay_buffer, batch_size=128)

        if done:
            break

    

    print(f"ğŸ¯ Reward: {episode_reward:.2f} | Steps: {step + 1} | Buffer: {replay_buffer.size}")
    
# âœ… æ‰€æœ‰ episodes è®­ç»ƒå®Œä¹‹åå†ç”»ä¸€æ¬¡å›¾
plt.figure()
plt.plot(all_rewards, label="Episode Reward")
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("TD3 Training Reward Curve")
plt.legend()
plt.grid()
plt.savefig("reward_curve.png")
print("âœ… Reward curve saved to reward_curve.png")
